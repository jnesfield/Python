{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Awesome Classification Metrics\n",
    "\n",
    "This notebook is some prebuilt things you need to validate classification models\n",
    "\n",
    "I will pepper in comments as to what to look for and references to help understand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn==1.0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import *\n",
    "from sklearn.calibration import calibration_curve, CalibrationDisplay\n",
    "from sklearn.dummy import DummyClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummy Predictions\n",
    "\n",
    "You need this to create dummy/no skill predictions to compare your model against."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_preds(x_train, y_train, x_test):\n",
    "    \"\"\"\n",
    "    this function creates a dummy model that has no skill but guesses probs and predictions and returns such for use later\n",
    "    input x_train, y_train and x_test pandas dataframes or numpy arrays\n",
    "    output preds and probs arrays showing predictions and probabilities for dummy classifier\n",
    "    \"\"\"\n",
    "    model = DummyClassifier(strategy = 'stratified')\n",
    "    model.fit(x_train, y_train)\n",
    "    preds = model.predict(x_test)\n",
    "    probs = model.predict_proba(x_test)\n",
    "    \n",
    "    return preds, probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix\n",
    "\n",
    "good reliable confusion matrix to see how the model performs\n",
    "\n",
    "this shows both count based and normalized confusion matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_df, preds):\n",
    "    \"\"\"\n",
    "    plots confusion matrix in both normalized and count formats\n",
    "    \n",
    "    \"\"\"\n",
    "    f, axes = plt.subplots(1, 2, figsize=(20, 5), sharey='row')\n",
    "\n",
    "    for i in range(2):\n",
    "        cf_matrix = confusion_matrix(y_df, preds)\n",
    "        if i == 0:\n",
    "            title = \"Normalized\"\n",
    "            cf_matrix = cf_matrix.astype('float') / cf_matrix.sum(axis = 1)[:,np.newaxis]\n",
    "        else:\n",
    "            title = \"Count Based\"\n",
    "        \n",
    "        disp = ConfusionMatrixDisplay(cf_matrix)\n",
    "        disp.plot(ax=axes[i])\n",
    "        disp.ax_.set_title(title)\n",
    "        disp.ax_.set_xlabel('Predicted Label')\n",
    "        if i!=0:\n",
    "            disp.ax_.set_ylabel('')\n",
    "\n",
    "    plt.subplots_adjust(wspace=0.40, hspace=0.1)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Metrics\n",
    "\n",
    "Run some standard metrics that you may need to run and also a classification report at the end.\n",
    "\n",
    "notes:\n",
    "- average precision score\n",
    "    - summarizes a precision-recall curve as the weighted mean of precisions achieved at each threshold\n",
    "- log loss\n",
    "    - indicative of how close the prediction probability is to the corresponding actual/true value (0 or 1 in case of binary classification)\n",
    "- precision score\n",
    "    - useful in situations where you want to capture true positives but dont care about false negatives or missed cases\n",
    "    - you are picking books in a library and want to make sure the books you pick are awesome and dont want any duds but also dont care if you miss some awesome books\n",
    "- recall score\n",
    "    - useful in situations where you want to maximize true positives and dont care about false positives\n",
    "    - you are robbing a house and grab as much jewelery as you can and dot care about gettting some party jewelery as long as you got all the good stuff you can sell\n",
    "- f1 score\n",
    "    - a balance btw precision and recall basically\n",
    "- classification report\n",
    "    - shows various metrics that are useful to see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_metrics(y_df, probs, preds):\n",
    "    \"\"\"\n",
    "    prints some classiffication metrics that are useful in model validation\n",
    "    \"\"\"\n",
    "    print(\"Average Precision Score: {:.4}\".format(average_precision_score(y_df, preds)))\n",
    "    print(\"Log Loss: {:.4}\".format(log_loss(y_df, probs[::,1])))    \n",
    "    print(\"Precision Score: {:.4}\".format(precision_score(y_df, preds)))\n",
    "    print(\"Recall Score: {:.4}\".format(recall_score(y_df, preds)))\n",
    "    print(\"F1 Score: {:.4}\".format(f1_score(y_df, preds)))\n",
    "    print(\"Classification Report: \\n\", classification_report(y_df, preds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auc Roc\n",
    "\n",
    "This shows how adjusting the probability threshold balances between trus positive and false positives\n",
    "\n",
    "We reuse the dummy predictions here labeled as input yhat to help plot how a no skill classifier compares to the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_auc_roc(y_df, probs, yhat):\n",
    "    \"\"\"\n",
    "    plots auc roc for model and no skill classifier\n",
    "    \"\"\"\n",
    "    \n",
    "    fpr, tpr, _ = roc_curve(y_df, probs[::,1])\n",
    "    n_fpr, n_tpr, _ = roc_curve(y_df, yhat[::,1])\n",
    "    \n",
    "    plt.plot(fpr, tpr, marker = '.', label = \"Trained Model\")\n",
    "    plt.plot(n_fpr, n_tpr, ':r', label = \"No Skill\")\n",
    "    \n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    \n",
    "    trained_auc = \"\\nTrained Model AUC ROC: {:4f}\".format(roc_auc_score(y_df, probs[::,1]))\n",
    "    no_skill_auc = \"\\nNo Skill AUC ROC: {:4f}\".format(roc_auc_score(y_df, yhat[::,1]))\n",
    "    plt.title(\"Auc Roc Plot and Scores:\" + trained_auc + no_skill_auc)\n",
    "    \n",
    "    plt.legend()\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision Recall Curve\n",
    "\n",
    "Shows the plot of the precision recall metrics as well as the no skill classifier and also shows the area under the curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import *\n",
    "\n",
    "def plot_precision_recall(y_df, probs, yhat):\n",
    "    \"\"\"\n",
    "    plots precision recall for model and no skill classifier\n",
    "    \"\"\"\n",
    "    \n",
    "    precision, recall, _ = precision_recall_curve(y_df, probs[::,1])\n",
    "    n_precision, n_recall, _ = precision_recall_curve(y_df, yhat[::,1])\n",
    "    \n",
    "    plt.plot(recall, precision, marker = '.', label = \"Trained Model\")\n",
    "    plt.plot(n_recall, n_precision, ':r', label = \"No Skill\")\n",
    "    \n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    \n",
    "    trained_auc = \"\\nTrained Model AUC: {:4f}\".format(auc(recall, precision))\n",
    "    no_skill_auc = \"\\nNo Skill AUC: {:4f}\".format(auc(n_recall, n_precision))\n",
    "    plt.title(\"Precision Recall Plot and Scores:\" + trained_auc + no_skill_auc)\n",
    "    \n",
    "    plt.legend()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calibration Plots\n",
    "\n",
    "These show how the model is performing in terms of calibration\n",
    "- Below the diagonal: The model has over-forecast; the probabilities are too large.\n",
    "- Above the diagonal: The model has under-forecast; the probabilities are too small.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot calibration(y_df, probs, yhat)\n",
    "    \"\"\"\n",
    "    prints calibration plots and brier scores for model v no skill classifier\n",
    "    \"\"\"\n",
    "    trained_brier = \"n\\Trained Model Brier Score: {:.4f}\".format(brier_score(y_df, probs[::,1]))\n",
    "    no_skill_brier = \"n\\No Skill Brier Score: {:.4f}\".format(brier_score(y_df, yhat[::,1]))\n",
    "    \n",
    "    fop, mpv = calibration_curve(y_df, probs[::,1], n_bins=10, normalize=True)\n",
    "    n_fop, n_mpv = calibration_curve(y_df, yhat[::,1], n_bins=10, normalize=True)\n",
    "    \n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', label = 'Prefect Calibration')\n",
    "    plt.plot(mpv, fop, marker='.', label = \"Trained Model\")\n",
    "    plt.plot(n_mpv, n_fop, marker='_', label = \"No Skill\")\n",
    "    \n",
    "    plt.title(\"Calibration Plots and Brier Scores:\" + trained_brier + no_skill_brier)\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
