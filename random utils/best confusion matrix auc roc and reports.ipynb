{"cells":[{"cell_type":"code","source":["#functions to show summary metrics for each model version:\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport itertools\nfrom sklearn.metrics import roc_curve, roc_auc_score\n\n\n#roc curve\ndef plot_roc(pred, test):\n  \n  \"\"\"\n  This function prints and plots the ROC.\n  \"\"\"\n  \n  #generate roc\n  fpr, tpr, _ = roc_curve(test, pred)\n  \n  print(\"ROC Model Performance:\")\n  print(\"true positive rate:\",tpr, \"\\nfalse positive rate:\",fpr)\n\n  x = np.linspace(0, 1, 1000)\n\n  # plot model roc curve\n  plt.plot(fpr, tpr, marker='.', label='Model')\n  plt.plot(x, x, ':r', label='Guess')\n\n  # axis labels\n  plt.xlabel('False Positive Rate')\n  plt.ylabel('True Positive Rate')\n  plt.title(('True Positive Rate:' + str(round(tpr,3)) + \"\\nFalse Positive Rate:\" + str(round(fpr,3)) + \"\\nAuc_Roc:\" + str(roc_auc_score(pred, y_test))))\n\n  # show the legend\n  plt.legend()\n\n  # show the plot\n  #display()\n\n\n#confusion matrix organized printing\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.tight_layout()\n    display()\n\n    \n\n#def function to pull in details we want\ndef model_perforance_check(pred, test):\n  \n  #checks model performance by creating a confusion matrix visualization and then running a classification report\n \n  #generate classification report\n  cr = classification_report(y_true =  test,  y_pred = pred)\n  #print classification report\n  print(\"Performance Report::\\n\", cr)\n  \n\n  #generate conf matrix\n  cm = confusion_matrix(y_true = test,  y_pred = pred)  \n  print(cm)\n  #plot confusion matrix\n  #plot_confusion_matrix(cm, ['False', 'True'])\n  #display()# This line of code enables using pre-built functions from another notebook\n  return cr\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"97a1e730-7da1-4a60-9eb0-3b1232f71079"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"best confusion matrix auc roc and reports","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":3036705700339708}},"nbformat":4,"nbformat_minor":0}
